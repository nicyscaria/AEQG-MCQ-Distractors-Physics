{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    " \n",
    "load_dotenv()\n",
    "\n",
    "TOGETHER_KEY = os.getenv('TOGETHER_API_KEY')\n",
    "OPENAI_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain_together import ChatTogether\n",
    " \n",
    "from typing import List\n",
    " \n",
    "topic_identifier_system = \"\"\"Analyze user input and identify the physics topic mentioned in the input. Do not return the input text.\n",
    "For example:\n",
    "- \"Create questions about velocity\" -> \"velocity\"\n",
    "- \"Explain displacement\" -> \"displacement\"\n",
    " \n",
    "Return only the identified physics topic name as given in the input.\"\"\"\n",
    " \n",
    "topic_check = ChatPromptTemplate.from_messages([\n",
    "   (\"system\", topic_identifier_system),\n",
    "   (\"placeholder\", \"{messages}\")\n",
    "]) | ChatTogether(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", temperature=0\n",
    "    )\n",
    " \n",
    "input = \"Create 5 questions on energy.\"\n",
    " \n",
    "topic = topic_check.invoke({\"messages\": [(\"user\", input)]}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'energy'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_together import ChatTogether\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import json\n",
    "\n",
    "system_prompt = (\n",
    "    \"Create one multiple choice question for {skill} level of Bloom's taxonomy for a 9th grade Physics student in India on {topic}.\"\n",
    "    \"Requirements:\"\n",
    "    \"Student should only be able to answer if they've mastered the concept\"\n",
    "    \"Each distractor must address either: A specific misconception about {topic} or A prerequisite knowledge gap\"\n",
    "    \"Language and complexity suitable for 9th grade\"\n",
    "    \"Physics context and application\"\n",
    "    \"Format as the output as a JSON:\"\n",
    " \n",
    "        \"\"\"{{{{\n",
    "        \"question\": \"\",\n",
    "        \"skill\": \"\"\n",
    "        \"options\": {{\"a\": \"\", \"b\": \"\", \"c\": \"\", \"d\": \"\"}},\n",
    "        \"correct\": \"\",\n",
    "        \"explanation\": {{\n",
    "            \"correct\": \"\",\n",
    "            \"a\": \"misconception/prerequisite tested\",\n",
    "            \"b\": \"\", \"c\": \"\", \"d\": \"\"\n",
    "        }}\n",
    "        }}}}\"\"\"\n",
    " \n",
    "    \"For {skill} level, ensure:\"\n",
    "    \"{skill_requirement}\"\n",
    "    \"Make sure there are no additional information being other than the output in the format that is asked for.\"\n",
    ")\n",
    "\n",
    "skill_requirements = {\n",
    "   \"Remember\": \"Question tests ability to retrieve relevant knowledge from long-term memory.\",\n",
    "   \"Understand\": \"Question tests ability to onstruct meaning from instructional messages, including oral, written, and graphic communication.\",\n",
    "   \"Apply\": \"Question tests ability to carry out or use a procedure in a given situation.\",\n",
    "   \"Analyze\": \"Question tests ability to break material into foundational parts and determine how parts relate to one another and the overall structure or purpose.\",\n",
    "   \"Evaluate\": \"Question tests ability to make judgments based on criteria and standards.\"\n",
    "}\n",
    " \n",
    "skills = [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\"]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\",\n",
    "#                  api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# llm = ChatTogether(\n",
    "#     model=\"Qwen/Qwen2.5-72B-Instruct-Turbo\"\n",
    "#     )\n",
    "\n",
    "llm = ChatTogether(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n",
    "    )\n",
    "\n",
    "# llm = ChatTogether(\n",
    "#     model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"\n",
    "#     )\n",
    "\n",
    "query = {\"input\": '{input}'}\n",
    "\n",
    "def generate_assessment(llm):\n",
    "   responses = {\n",
    "       \"topic\": topic,\n",
    "       \"questions\": []\n",
    "   }\n",
    "   \n",
    "   for skill in skills:\n",
    "\n",
    "       prompt = system_prompt.format(\n",
    "           skill=skill,\n",
    "           topic=topic,\n",
    "           skill_requirement=skill_requirements[skill]\n",
    "       )\n",
    "\n",
    "       response = llm.invoke(prompt)\n",
    "       cleaned_content = response.content.strip()\n",
    "       if cleaned_content.startswith(\"```json\"):\n",
    "           cleaned_content = cleaned_content[7:-3]\n",
    "           \n",
    "       try:\n",
    "           question_json = json.loads(cleaned_content)\n",
    "           responses[\"questions\"].append(question_json)\n",
    "       except json.JSONDecodeError as e:\n",
    "           print(response)\n",
    "           print(f\"Error parsing {skill} response\")\n",
    "   \n",
    "   model_name = llm.model_name.split('/')[-1]\n",
    "   filename = f\"LLM_{topic}_{model_name}.json\"\n",
    "   \n",
    "   with open(filename, \"w\") as f:\n",
    "       json.dump(responses, f, indent=2, ensure_ascii=False)\n",
    "       \n",
    "   return responses\n",
    " \n",
    "final_assessment = generate_assessment(llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auro_mcq_distractors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating vector store chroma_db_huggingface ---\n",
      "--- Finished creating vector store chroma_db_huggingface ---\n",
      "Embedding demonstrations for Hugging Face completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "try:\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # Fallback for environments where __file__ is not defined\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "file_path = os.path.join(current_dir, \"books\", \"OpenStax_Physics_Required.pdf\")\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"The file {file_path} does not exist. Please check the path.\"\n",
    "    )\n",
    "\n",
    "# Read the text content from the file\n",
    "loader = PyPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "def create_vector_store(docs, embeddings, store_name):\n",
    "    persistent_directory = os.path.join(db_dir, store_name)\n",
    "    if not os.path.exists(persistent_directory):\n",
    "        print(f\"\\n--- Creating vector store {store_name} ---\")\n",
    "        Chroma.from_documents(\n",
    "            docs, embeddings, persist_directory=persistent_directory)\n",
    "        print(f\"--- Finished creating vector store {store_name} ---\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Vector store {store_name} already exists. No need to initialize.\")\n",
    "        \n",
    "\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "create_vector_store(docs, huggingface_embeddings, \"chroma_db_huggingface\")\n",
    "\n",
    "print(\"Embedding demonstrations for Hugging Face completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    " \n",
    "load_dotenv()\n",
    "\n",
    "TOGETHER_KEY = os.getenv('TOGETHER_API_KEY')\n",
    "# OPENAI_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain_together import ChatTogether\n",
    " \n",
    "from typing import List\n",
    " \n",
    "topic_identifier_system = \"\"\"Analyze user input and identify the physics topic mentioned in the input. Do not return the input text.\n",
    "For example:\n",
    "- \"Create questions about velocity\" -> \"velocity\"\n",
    "- \"Explain displacement\" -> \"displacement\"\n",
    " \n",
    "Return only the identified physics topic name as given in the input.\"\"\"\n",
    " \n",
    "topic_check = ChatPromptTemplate.from_messages([\n",
    "   (\"system\", topic_identifier_system),\n",
    "   (\"placeholder\", \"{messages}\")\n",
    "]) | ChatTogether(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", temperature=0\n",
    "    )\n",
    " \n",
    "input = \"Create 5 questions on displacement.\"\n",
    " \n",
    "topic = topic_check.invoke({\"messages\": [(\"user\", input)]}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'displacement'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remember\n",
      "\n",
      "--- Querying the Vector Store chroma_db_huggingface ---\n",
      "Understand\n",
      "\n",
      "--- Querying the Vector Store chroma_db_huggingface ---\n",
      "Apply\n",
      "\n",
      "--- Querying the Vector Store chroma_db_huggingface ---\n",
      "Analyze\n",
      "\n",
      "--- Querying the Vector Store chroma_db_huggingface ---\n",
      "Evaluate\n",
      "\n",
      "--- Querying the Vector Store chroma_db_huggingface ---\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_together import ChatTogether\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import json\n",
    "\n",
    "def query_vector_store(store_name, query, embedding_function):\n",
    "    persistent_directory = os.path.join(db_dir, store_name)\n",
    "    if os.path.exists(persistent_directory):\n",
    "        print(f\"\\n--- Querying the Vector Store {store_name} ---\")\n",
    "        db = Chroma(\n",
    "            persist_directory=persistent_directory,\n",
    "            embedding_function=embedding_function,\n",
    "        )\n",
    "        retriever = db.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 3},\n",
    "        )\n",
    "        # Display the relevant results with metadata\n",
    "        return retriever\n",
    "\n",
    "system_prompt = (\n",
    "    \"Create one multiple choice question for {skill} level of Bloom's taxonomy for a 9th grade Physics student in India on {topic}.\"\n",
    "    \"Requirements:\"\n",
    "    \"Student should only be able to answer if they've mastered the concept\"\n",
    "    \"Each distractor must address either: A specific misconception about {topic} or A prerequisite knowledge gap\"\n",
    "    \"Language and complexity suitable for 9th grade\"\n",
    "    \"Physics context and application\"\n",
    "    \"Use {context} for accuracy in creating the questions and distractors.\"\n",
    "    \"Format as the output as a JSON:\"\n",
    " \n",
    "        \"\"\"{{{{\n",
    "        \"question\": \"\",\n",
    "        \"skill\": \"\"\n",
    "        \"options\": {{\"a\": \"\", \"b\": \"\", \"c\": \"\", \"d\": \"\"}},\n",
    "        \"correct\": \"\",\n",
    "        \"explanation\": {{\n",
    "            \"correct\": \"\",\n",
    "            \"a\": \"misconception/prerequisite tested\",\n",
    "            \"b\": \"\", \"c\": \"\", \"d\": \"\"\n",
    "        }}\n",
    "        }}}}\"\"\"\n",
    " \n",
    "    \"For {skill} level, ensure:\"\n",
    "    \"{skill_requirement}\"\n",
    "    \"Make sure there are no additional information being other than the output in the format that is asked for.\"\n",
    ")\n",
    "\n",
    "skill_requirements = {\n",
    "   \"Remember\": \"Question tests ability to retrieve relevant knowledge from long-term memory.\",\n",
    "   \"Understand\": \"Question tests ability to onstruct meaning from instructional messages, including oral, written, and graphic communication.\",\n",
    "   \"Apply\": \"Question tests ability to carry out or use a procedure in a given situation.\",\n",
    "   \"Analyze\": \"Question tests ability to break material into foundational parts and determine how parts relate to one another and the overall structure or purpose.\",\n",
    "   \"Evaluate\": \"Question tests ability to make judgments based on criteria and standards.\"\n",
    "}\n",
    " \n",
    "skills = [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\"]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\",\n",
    "#                  api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# llm = ChatTogether(\n",
    "#     model=\"Qwen/Qwen2.5-72B-Instruct-Turbo\"\n",
    "#     )\n",
    "\n",
    "llm = ChatTogether(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n",
    "    )\n",
    "\n",
    "# llm = ChatTogether(\n",
    "#     model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"\n",
    "#     )\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
    "#                  api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "query = {\"input\": '{input}'}\n",
    "\n",
    "def generate_assessment(topic, llm):\n",
    "   responses = {\n",
    "       \"topic\": topic,\n",
    "       \"questions\": []\n",
    "   }\n",
    "   \n",
    "   for skill in skills:\n",
    "       \n",
    "       print(skill)\n",
    "       retriever = query_vector_store(\"chroma_db_huggingface\", query['input'], huggingface_embeddings)\n",
    "       documents = retriever.invoke(query['input'])\n",
    "       \n",
    "       # Combine the retrieved documents into a single context string\n",
    "       context = \" \".join(doc.page_content for doc in documents)\n",
    "\n",
    "       prompt = system_prompt.format(\n",
    "           skill=skill,\n",
    "           topic=topic,\n",
    "           context = context,\n",
    "           skill_requirement=skill_requirements[skill]\n",
    "       )\n",
    "\n",
    "       response = llm.invoke(prompt)\n",
    "       cleaned_content = response.content.strip()\n",
    "       if cleaned_content.startswith(\"```json\"):\n",
    "           cleaned_content = cleaned_content[7:-3]\n",
    "           \n",
    "       try:\n",
    "           question_json = json.loads(cleaned_content)\n",
    "           responses[\"questions\"].append(question_json)\n",
    "       except json.JSONDecodeError as e:\n",
    "           print(response)\n",
    "           print(f\"Error parsing {skill} response\")\n",
    "   \n",
    "   model_name = llm.model_name.split('/')[-1]\n",
    "   filename = f\"RAG_{topic}_{model_name}.json\"\n",
    "   \n",
    "   with open(filename, \"w\") as f:\n",
    "       json.dump(responses, f, indent=2, ensure_ascii=False)\n",
    "       \n",
    "   return responses\n",
    " \n",
    "final_assessment = generate_assessment(topic, llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auro_mcq_distractors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

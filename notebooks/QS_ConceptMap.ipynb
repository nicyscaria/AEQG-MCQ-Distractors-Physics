{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This ipynb notebook can be used to create questions for different topics  at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection parameters\n",
    "DATABASE = os.getenv('DB_NAME')\n",
    "USER = os.getenv('DB_USER')\n",
    "PASSWORD = os.getenv('DB_PASSWORD')\n",
    "HOST = os.getenv('DB_HOST')\n",
    "PORT = os.getenv('DB_PORT')\n",
    "\n",
    "TOGETHER_KEY = os.getenv('TOGETHER_API_KEY')\n",
    "OPENAI_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "postgres_uri = f\"postgresql://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\"\n",
    "\n",
    "db = SQLDatabase.from_uri(postgres_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from chat_together import ChatTogether\n",
    "\n",
    "from typing import List\n",
    "\n",
    "@tool \n",
    "def get_topic_tool(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze user input and identify the physics topic mentioned in the input.\n",
    "    \"\"\"\n",
    "    topic_identifier_system = \"\"\"Analyze user input and identify the physics topic mentioned in the input. Do not return the input text.\n",
    "        For example:\n",
    "        - \"Create questions about velocity\" -> \"velocity\"\n",
    "        - \"Explain displacement\" -> \"displacement\"\n",
    "        \n",
    "        Return only the identified physics topic name as given in the input.\n",
    "        \n",
    "        Make sure not to add anything extra to the output.\"\"\"\n",
    "        \n",
    "    topic_check = ChatPromptTemplate.from_messages([\n",
    "                        (\"system\", topic_identifier_system),\n",
    "                        (\"placeholder\", \"{messages}\")\n",
    "                    ]) | ChatTogether(\n",
    "                            model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", temperature=0\n",
    "                        )\n",
    "\n",
    "    topic = topic_check.invoke({\"messages\": [(\"user\", input)]}).content\n",
    "    \n",
    "    return topic\n",
    "\n",
    "input = \"Give your input here to create questions\"\n",
    "\n",
    "topic = get_topic_tool(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_together import ChatTogether\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "@tool\n",
    "def find_matching_topic_id(topic_of_interest: str) -> str:\n",
    "    \"\"\"\n",
    "    Find most matching topic using LLM reasoning.\n",
    "    \"\"\"\n",
    "    # Get all topics from database\n",
    "    query = \"SELECT topic_name, topic_id FROM topics;\"\n",
    "    result = db.run_no_throw(query)\n",
    "    \n",
    "    if not result:\n",
    "        return \"No topics found in database\"\n",
    "    \n",
    "    topics = result\n",
    "    \n",
    "    # Initialize LLM\n",
    "    llm = ChatTogether(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", temperature=0\n",
    "    )\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"You are a hierarchical topic matching expert. Find the most relevant topic from the available list.\n",
    "\n",
    "            Topic to match: {topic_of_interest}\n",
    "\n",
    "            Available topics:\n",
    "            {topics}\n",
    "\n",
    "            Instructions:\n",
    "            \n",
    "            Match the topic by considering:\n",
    "                - Exact matches\n",
    "                - Parent concepts that encompass the given topic\n",
    "                - Fundamental principles that explain or govern the topic\n",
    "                - Related physical phenomena or laws that include this topic\n",
    "                \n",
    "                For example:\n",
    "                    - \"inertia\" should match with \"Newton's First Law of Motion\"\n",
    "                    - \"weight\" should match with \"Newton's Second Law of Motion\"\n",
    "                    - \"action-reaction\" should match with \"Newton's Third Law of Motion\"\n",
    "            \n",
    "            Return ONLY the exact matching topic_ID as it is without adding additional space without any additional information.\n",
    "\n",
    "            If you find more than one matching topic, return the one that comes first.\n",
    "\n",
    "            If none match well, return 'NO_MATCH'.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Create chain and run\n",
    "    chain = prompt | llm\n",
    "    \n",
    "    response = chain.invoke({\n",
    "        \"topic_of_interest\": topic_of_interest,\n",
    "        \"topics\": \"\\n\".join(topics)\n",
    "    }).content\n",
    "    \n",
    "    return response\n",
    "\n",
    "matched_topic_id = find_matching_topic_id(topic)\n",
    "print(matched_topic_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"WITH topic_search AS \n",
    "(SELECT topic_id FROM topics \n",
    "WHERE topic_id ILIKE '{matched_topic_id}')\n",
    "SELECT s.subtopic_name, \n",
    "s.description, \n",
    "s.mathematical_formulation,\n",
    "s.prerequisites,\n",
    "s.misconceptions,\n",
    "s.engineering_applications,\n",
    "s.cross_cutting_topics,\n",
    "s.analogies\n",
    "FROM subtopics s \n",
    "JOIN topic_search ts \n",
    "ON s.topic_id = ts.topic_id \n",
    "LIMIT 10;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "# this tool can run a SQL query and get the output\n",
    "\n",
    "@tool\n",
    "def db_query_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Execute a SQL query against the database and get back the result.\n",
    "    If the query is not correct, an error message will be returned.\n",
    "    If an error is returned, rewrite the query, check the query, and try again.\n",
    "    \"\"\"\n",
    "    result = db.run(query)\n",
    "    if not result:\n",
    "        return \"Error: Query failed. Please rewrite your query and try again.\"\n",
    "    return result\n",
    "\n",
    "context = db_query_tool(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionGenerator:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.base_prompt = \"\"\"\n",
    "        \n",
    "        Create one multiple choice question for {skill} level of Bloom's taxonomy for a 9th grade Physics student in India on {topic}. \n",
    "\n",
    "        Use {context} for accuracy in creating the questions and distractors. \n",
    "\n",
    "        Specifically use information from the prerequisites, misconceptions, engineering_applications, cross_cutting_topics, analogies for the generation.\n",
    "        \n",
    "        Make sure you don't specify the topic in the question, like 'according to Newton's first law of motion' or 'according to work-energy theorem'.\n",
    "\n",
    "        Requirements:\n",
    "\n",
    "        - Student should only be able to answer if they've mastered the concept\"\n",
    "        - Each distractor must address either: \n",
    "            - A specific misconception about {topic} \n",
    "            - A prerequisite knowledge gap\"\n",
    "        - Language and complexity suitable for 9th grade\n",
    "        - Unique question, don't generate questions that are in or similar to questions in question history - {question_history}\n",
    "        - Accurate answer and not ambiguous\n",
    "        - Physics context and application\n",
    "\n",
    "        Make sure to include the question, answer, distractors, misconceptions, and explanation in the output in a JSON format.\n",
    "        \n",
    "        Output format:\n",
    "        \n",
    "        {{{{\n",
    "            \"question\": \"\",\n",
    "            \"skill\": \"\"\n",
    "            \"options\": {{\"a\": \"\", \"b\": \"\", \"c\": \"\", \"d\": \"\"}},\n",
    "            \"correct\": \"\",\n",
    "            \"explanation\": {{\n",
    "                \"correct\": \"\",\n",
    "                \"a\": \"misconception/prerequisite tested\",\n",
    "                \"b\": \"\", \"c\": \"\", \"d\": \"\"\n",
    "            }}\n",
    "        }}}}\n",
    "        \n",
    "        For {skill} level, ensure that the question meets the skill requirement: {skill_requirement}.\n",
    "\n",
    "        Make sure there are no additional information being other than the output in the format that is asked for.\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "    def generate_question(self, skill, skill_requirement, topic, context, question_history):\n",
    "        prompt = self.base_prompt.format(\n",
    "            skill=skill,\n",
    "            skill_requirement=skill_requirement,\n",
    "            topic=topic,\n",
    "            context=context,\n",
    "            question_history=question_history\n",
    "        )\n",
    "        response = self.llm.invoke(prompt)\n",
    "        cleaned_content = response.content.strip()\n",
    "        question_json = json.loads(cleaned_content)\n",
    "        return question_json\n",
    "\n",
    "class QuestionEvaluator:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.evaluation_prompt = \"\"\"\n",
    "        Evaluate the question meticulously:\n",
    "\n",
    "        Question to evaluate:\n",
    "        {question}\n",
    "\n",
    "        Evaluate the question based on the following criteria:\n",
    "        \n",
    "        1. Uniqueness Check:\n",
    "           - Compare with previous questions: {previous_questions}\n",
    "           - If there are no previous questions, mark as unique\n",
    "           - Check for similar concepts, context, or wording\n",
    "           - Verify different application/scenario\n",
    "           \n",
    "        2. Answer Check:\n",
    "            - Correct answer must be unique and accurate\n",
    "            - Explanation must be clear and concise\n",
    "            \n",
    "        If the question give a unique question that is not present before and the right answer for the question among the options, return a JSON object with \"valid\": true. \n",
    "        \n",
    "        Otherwise, return a JSON object with \"valid\": false in the format:\n",
    "        \n",
    "        {{{{\n",
    "            \"valid\": True/False,\n",
    "            \"1\": {{\n",
    "                \"uniqueness\": True/False,\n",
    "                \"uniqueness_issues\": \" \" # the issues for False, else None \n",
    "            }},\n",
    "            \"2\": {{\n",
    "                \"answer\": True/False,\n",
    "                \"answer_issues\": \" \" # the issues for False, else None\n",
    "            }}\n",
    "        }}}}\n",
    "        \n",
    "        Make sure there are no additional information being other than the output in the format that is asked for.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "    def evaluate_question(self, question, previous_questions):\n",
    "       prompt = self.evaluation_prompt.format(\n",
    "           question=json.dumps(question),\n",
    "           previous_questions=json.dumps(previous_questions), \n",
    "           skill=question[\"skill\"]\n",
    "       )\n",
    "       try:\n",
    "           response = self.llm.invoke(prompt).content\n",
    "           evaluation = json.loads(response)\n",
    "       except:\n",
    "           evaluation = {\n",
    "               \"valid\": False,\n",
    "               \"1\": {\"uniqueness\": False, \"uniqueness_issues\": \"Failed to evaluate\"},\n",
    "               \"2\": {\"answer\": False, \"answer_issues\": \"Failed to evaluate\"},\n",
    "               \"3\": {\"distractor\": False, \"distractor_issues\": \"Failed to evaluate\"},\n",
    "               \"4\": {\"grade_level\": False, \"grade_issues\": \"Failed to evaluate\"},\n",
    "               \"5\": {\"skill_alignment\": False, \"skill_issues\": \"Failed to evaluate\"}\n",
    "           }\n",
    "       return evaluation\n",
    "   \n",
    "class QuestionFixer:\n",
    "   def __init__(self, llm):\n",
    "       self.llm = llm\n",
    "       self.fix_prompts = {\n",
    "           \"uniqueness\": \"\"\"\n",
    "           Current question has uniqueness issue: {question}\n",
    "           Previous questions: {previous_questions}\n",
    "           \n",
    "           Create new {skill} level question about {topic} that is distinctly different.\n",
    "           Must use exact JSON format as current question.\n",
    "           \"\"\",\n",
    "           \n",
    "           \"answer\": \"\"\"\n",
    "           Question with answer issue: {question}\n",
    "           \n",
    "           Modify only:\n",
    "           1. Correct answer option\n",
    "           2. Correct answer explanation\n",
    "           \n",
    "           Return in same JSON format with only these changes.\"\"\"\n",
    "       }\n",
    "\n",
    "   def fix_question(self, question, evaluation, previous_questions=None):\n",
    "       if not evaluation[\"1\"][\"uniqueness\"]:\n",
    "           return self._generate_new_question(question, previous_questions)\n",
    "       elif not evaluation[\"2\"][\"answer\"]:\n",
    "           return self._fix_answer(question)\n",
    "       return question\n",
    "\n",
    "   def _generate_new_question(self, question, previous):\n",
    "       prompt = self.fix_prompts[\"uniqueness\"].format(\n",
    "           question=json.dumps(question),\n",
    "           previous_questions=previous,\n",
    "           skill=question[\"skill\"],\n",
    "           topic=topic\n",
    "       )\n",
    "       response = self.llm.invoke(prompt)\n",
    "       return json.loads(response.content)\n",
    "\n",
    "   def _fix_answer(self, question):\n",
    "       prompt = self.fix_prompts[\"answer\"].format(\n",
    "           question=json.dumps(question)\n",
    "       )\n",
    "       updated = json.loads(self.llm.invoke(prompt).content)\n",
    "       question[\"correct\"] = updated[\"correct\"]\n",
    "       question[\"explanation\"][\"correct\"] = updated[\"explanation\"][\"correct\"]\n",
    "       return question\n",
    "\n",
    "\n",
    "class AssessmentGenerator:\n",
    "    def __init__(self, llm, skills, skill_requirements):\n",
    "        self.generator = QuestionGenerator(llm)\n",
    "        self.evaluator = QuestionEvaluator(llm)\n",
    "        self.fixer = QuestionFixer(llm)\n",
    "        self.skills = skills\n",
    "        self.skill_requirements = skill_requirements\n",
    "    \n",
    "    def generate_assessment(self, topic, context):\n",
    "        assessment = {\n",
    "            \"topic\": topic,\n",
    "            \"questions\": []\n",
    "        }\n",
    "        \n",
    "        for skill in self.skills:\n",
    "            skill_requirement = self.skill_requirements[skill]\n",
    "            print(f\"\\n=== Generating {skill} question ===\")\n",
    "            question = self._generate_valid_question(skill, skill_requirement, topic, context, assessment[\"questions\"])\n",
    "            if question:\n",
    "                assessment[\"questions\"].append(question)\n",
    "                print(f\"Successfully generated {skill} question\")\n",
    "            else:\n",
    "                print(f\"Failed to generate valid {skill} question after max attempts\")\n",
    "        \n",
    "        return assessment\n",
    "\n",
    "    def _generate_valid_question(self, skill, skill_requirement, topic, context, previous_questions, max_attempts=2):\n",
    "        for attempt in range(max_attempts):\n",
    "            print(f\"\\nAttempt {attempt + 1}/{max_attempts}\")\n",
    "            question_history = [q['question'] for q in previous_questions] if previous_questions else []\n",
    "            try:\n",
    "                question = self.generator.generate_question(skill, skill_requirement, topic, context, question_history)\n",
    "                evaluation = self.evaluator.evaluate_question(question, question_history)\n",
    "                \n",
    "                if evaluation.get(\"valid\", False):\n",
    "                    return question\n",
    "                \n",
    "                while not evaluation[\"valid\"]:\n",
    "                    fixed_question = self.fixer.fix_question(question, evaluation, question_history)\n",
    "                    fixed_evaluation = self.evaluator.evaluate_question(fixed_question, question_history)\n",
    "                    \n",
    "                    if fixed_evaluation[\"valid\"]:\n",
    "                        return fixed_question\n",
    "                    \n",
    "                    if fixed_question == question:\n",
    "                        break\n",
    "                    \n",
    "                    question = fixed_question\n",
    "                    evaluation = fixed_evaluation\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "                continue\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "llm = ChatTogether(model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", temperature = 0.75)\n",
    "skills = [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\"]\n",
    "\n",
    "skill_requirements = {\n",
    "   \"Remember\": \"Question tests ability to retrieve relevant knowledge from long-term memory.\",\n",
    "   \"Understand\": \"Question tests ability to onstruct meaning from instructional messages, including oral, written, and graphic communication.\",\n",
    "   \"Apply\": \"Question tests ability to carry out or use a procedure in a given situation.\",\n",
    "   \"Analyze\": \"Question tests ability to break material into foundational parts and determine how parts relate to one another and the overall structure or purpose.\",\n",
    "   \"Evaluate\": \"Question tests ability to make judgments based on criteria and standards.\"\n",
    "}\n",
    "\n",
    "topics = [ ] # Add topics here\n",
    "\n",
    "def generate_assessment(llm):\n",
    "   all_responses = [] \n",
    "   model_name = llm.model_name.split('/')[-1]\n",
    "   \n",
    "   for topic in topics:\n",
    "      print(f\"\\n=== Generating assessment for {topic} ===\")\n",
    "      \n",
    "      generator = AssessmentGenerator(llm, skills, skill_requirements)\n",
    "      assessment = generator.generate_assessment(topic)\n",
    "      \n",
    "      json_filename = f\"LLM_{topic}_{model_name}.json\"\n",
    "      with open(json_filename, \"w\") as f:\n",
    "         json.dump(assessment, f, indent=2, ensure_ascii=False)\n",
    "      all_responses.extend(\n",
    "         {\"topic\": topic, **question} \n",
    "         for question in assessment[\"questions\"]\n",
    "      )\n",
    "   \n",
    "   # Convert all responses to CSV\n",
    "   csv_data = []\n",
    "   max_options = 0\n",
    "    \n",
    "    # First pass to find maximum number of options across all questions\n",
    "   for response in all_responses:\n",
    "      max_options = max(max_options, len(response.get(\"options\", [])))\n",
    "   \n",
    "   # Second pass to create rows\n",
    "   for response in all_responses:\n",
    "      options = response.get(\"options\", {})\n",
    "      explanations = response.get(\"explanation\", {})\n",
    "      \n",
    "      row = {\n",
    "         \"topic\": response[\"topic\"],\n",
    "         \"skill\": response.get(\"skill\", \"\"),\n",
    "         \"question\": response.get(\"question\", \"\"),\n",
    "         \"correct_option\": response.get(\"correct\", \"\"),\n",
    "         \"correct_explanation\": explanations.get(\"correct\", \"\")\n",
    "      }\n",
    "      \n",
    "      # Add option and explanation columns for each letter\n",
    "      for letter in ['a', 'b', 'c', 'd']:\n",
    "         row[f\"option_{letter}\"] = options.get(letter, \"\")\n",
    "         row[f\"explanation_{letter}\"] = explanations.get(letter, \"\")\n",
    "      \n",
    "      csv_data.append(row)\n",
    "   \n",
    "   # Save consolidated CSV\n",
    "   csv_filename = f\"LLM_all_topics_{model_name}.csv\"\n",
    "   if csv_data:\n",
    "      df = pd.DataFrame(csv_data)\n",
    "      df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_assessment(llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auro_mcq_distractors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
